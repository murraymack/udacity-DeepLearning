## Chapter 2: Recurrent Neural Networks (RNNs)

### Lesson 1: Introduction to Recurrent Neural Networks
1. [Generate a descriptive caption](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)
2. [Sketch RNN Demo](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html)
3. [Vanishing Gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
4. [OpenAI: DotA 2 Bot](https://blog.openai.com/dota-2/)
5. [Adding sounds to silent movies](https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8)
6. [Handwriting Generation](http://www.cs.toronto.edu/~graves/handwriting.cgi?text=My+name+is+Luka&style=&bias=0.15&samples=3)
7. [Amazon: Lex](https://aws.amazon.com/lex/faqs/)
8. [Facebook: Building Language Models](https://code.facebook.com/posts/1827693967466780/building-an-efficient-neural-language-model-over-a-billion-words/)
9. [Netflix RNNs](https://arxiv.org/pdf/1511.06939.pdf)
10. [Activation Functions Refresher](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)
11. [cs231 Andrej Karpathy, ReLUs can die](https://cs231n.github.io/neural-networks-1/#nn)
12. [Cross Entropy, Mathy](https://www.ics.uci.edu/~pjsadows/notes.pdf)
13. [Rules of Calculus](http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html)
14. [Common Derivatives Cheatsheet](http://tutorial.math.lamar.edu/pdf/Common_Derivatives_Integrals.pdf)
15. [Tuning Learning Rates in Gradient Descent](http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/)
16. ['Bold Driver' Gradient Descent Optimization](https://cnl.salk.edu/~schraudo/teach/NNcourse/momrate.html)
17. [cs231 Andrej Karpathy, Tuning Learning Rates](https://cnl.salk.edu/~schraudo/teach/NNcourse/momrate.html)
18. 
